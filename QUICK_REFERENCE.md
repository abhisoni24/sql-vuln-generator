# Quick Reference Card

## âš¡ 60-Second Quick Start

### 1. Get API Keys (5 minutes)
```bash
# Claude: https://console.anthropic.com/account/keys
# OpenAI: https://platform.openai.com/account/api-keys
```

### 2. Configure (1 minute)
```bash
cp .env.example .env
# Edit .env with your API keys
nano .env
```

### 3. Verify Setup (30 seconds)
```bash
python setup_check.py
```

### 4. Run Experiment (2-3 minutes)
```bash
python -m src.experiment_main
```

### 5. View Results
```bash
cd experiments
cat REPORT.md          # Markdown report
open *.png             # Visualizations
cat experiment_results.json  # Raw data
```

---

## ğŸ”‘ Most Common Commands

```bash
# Full setup
make setup

# Run with different sizes
make run-small         # 5 samples (fast)
make run              # 20 samples (default)
make run-large        # 100 samples (comprehensive)

# Analysis
make analyze          # Analyze latest results
make clean            # Clean temporary files
```

---

## ğŸ“Š What Gets Generated

```
experiments/
â”œâ”€â”€ experiment_results.json      # Raw data (JSON)
â”œâ”€â”€ REPORT.md                    # Detailed report (Markdown)
â”œâ”€â”€ 01_vulnerability_distribution.png    # Pie chart
â”œâ”€â”€ 02_cwe_distribution.png              # Bar chart
â”œâ”€â”€ 03_summary_statistics.png            # Statistics
â””â”€â”€ 04_sample_showcase.png               # Examples
```

---

## ğŸ› Quick Fixes

### API Key Not Found
```bash
# Check .env file exists
ls -la .env

# Fix: Create from template
cp .env.example .env
nano .env  # Add your keys
```

### Dependencies Missing
```bash
pip install -r requirements.txt
```

### Network/Rate Limit Issues
```bash
# Try fewer samples
python -m src.experiment_main 5

# Wait a minute, then try again
```

### ModuleNotFoundError
```bash
# Must use -m flag and run from project root
python -m src.experiment_main  # âœ“ Correct
python src/experiment_main.py   # âœ— Wrong
```

---

## ğŸ“ˆ Understanding Results

### Verdict Meanings
- **VULNERABLE** = SQL injection detected
- **NOT VULNERABLE** = Code appears safe
- **ERROR** = Analysis failed

### Key Metrics
- Vulnerability % = Vulnerable count / Total count
- CWE-89 = Most common (SQL injection)

### If You See:
- **>50% vulnerable** â†’ GPT-3.5 generates risky SQL
- **<30% vulnerable** â†’ Generally safe code
- **10+ CWEs found** â†’ Many vulnerability types

---

## ğŸ’° Estimated Costs

Per 20 samples: **$0.03 - $0.06**

- 20 samples: ~$0.05
- 50 samples: ~$0.12
- 100 samples: ~$0.25
- 500 samples: ~$1.25

---

## ğŸ“š Documentation Map

Start â†’ **This File** (you are here)
         â†“
Setup Help â†’ `SETUP_GUIDE.md`
         â†“
Run Experiment â†’ `python -m src.experiment_main`
         â†“
View Results â†’ `experiments/REPORT.md`
         â†“
Deep Dive â†’ `PROJECT_README.md` or `experiments/README.md`

---

## ğŸ¯ Typical Workflow

```
1. python setup_check.py          (Verify setup - 30s)
2. python -m src.experiment_main  (Run experiment - 2-3 min)
3. cd experiments && open REPORT.md        (Review findings)
4. python -m src.utils.analysis experiments/experiment_results.json  (Deep analysis)
```

---

## ğŸ”— Quick Links

| Item | Where |
|------|-------|
| API Keys | https://console.anthropic.com/account/keys (Claude) |
| | https://platform.openai.com/account/api-keys (OpenAI) |
| Claude Docs | https://docs.anthropic.com/ |
| OpenAI Docs | https://platform.openai.com/docs/ |
| CWE Info | https://cwe.mitre.org/data/definitions/89.html |

---

## âš ï¸ Important Notes

- **Never commit .env file** (has API keys!)
- **It's in .gitignore** (safe by default)
- **Delete experiment results before pushing**: `make clean-results`
- **Test with small samples first**: `make run-small`

---

## ğŸš€ Pro Tips

### Run Multiple Experiments
```bash
python -m src.experiment_main 20
mv experiments/experiment_results.json experiments/run1.json
python -m src.experiment_main 20
# Now have run1.json and latest results
```

### Export for Excel
```python
import pandas as pd
import json
with open("experiments/experiment_results.json") as f:
    df = pd.DataFrame(json.load(f))
df.to_excel("results.xlsx", index=False)
```

### Compare Experiments
```python
from src.utils.analysis import ExperimentAnalyzer
a1 = ExperimentAnalyzer("experiments/run1.json")
a2 = ExperimentAnalyzer("experiments/experiment_results.json")
print(a1.generate_comparison_report(a2))
```

---

## ğŸ“ Need Help?

1. **Setup issues** â†’ Run `python setup_check.py`
2. **API errors** â†’ Check API status pages
3. **Documentation** â†’ Read `SETUP_GUIDE.md`
4. **Deep dive** â†’ Read `PROJECT_README.md`
5. **Results confused** â†’ Check `experiments/README.md`

---

**Remember:** Start simple with `make setup && make run-small` to test!
