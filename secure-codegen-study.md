Based on the available literature, the techniques described for the manipulation and optimization of prompts for safe code generation generally fall under **three overarching trends**: 1) **Retrieval-Augmented Augmentation**, where external security knowledge is dynamically inserted into the prompt; 2) **Iterative Refinement**, where prompts are generated or mutated based on feedback from code evaluation or review; and 3) **Direct Model Instruction Tuning**, where the core capabilities of the LLM are trained to prioritize security in response to a prompt.

Below is a summary of the techniques, categorized by these trends:

***

### General Trend 1: Retrieval-Augmented Prompt Augmentation (RAG)

This trend involves dynamically enriching the input prompt with relevant, curated security information retrieved from an external knowledge base to guide the Large Language Model (LLM) toward generating secure code.

| Technique | Mechanism of Prompt Manipulation | Key Components for Safety |
| :--- | :--- | :--- |
| **RESCUE** (REtrieval-augmented Secure Code gEneration) | Retrieved security knowledge forms a **tailored security context** that augments the prompt to steer the LLM. | **Hybrid Knowledge Base:** Combines high-level security guidelines (distilled via LLM-assisted cluster-then-summarize) and concise, security-focused code examples (extracted via static program slicing). **Hierarchical Multi-Faceted Retrieval:** Proactively analyzes the task description using three security-critical facets—**Vulnerability Cause Analysis, Draft Code Generation, and API Call Extraction**—to select precise security knowledge for augmentation. |
| **RAG** (in "Retrieve, Refine or Both?") | Dynamically retrieves the top 'n' most relevant **task-specific secure coding guidelines** from the *SecGuide* database (comprising guidelines from MITRE CWE and CodeQL) and appends them to the coding task input. | **SecGuide Database:** Consists of 320 secure coding guidelines, each associated with preconditions defining when the guideline is relevant. **Cosine Similarity Retrieval:** Calculates cosine similarity between the task embedding and stored guideline embeddings to retrieve relevant information, ensuring the guidelines are applicable to the code’s functionality. |

***

### General Trend 2: Iterative Feedback and Refinement

This trend utilizes an iterative loop where the LLM generates code, the code is analyzed or evaluated, and the resulting feedback is used to generate or refine a subsequent prompt, leading to incremental security improvements.

| Technique | Mechanism of Prompt Manipulation | Key Components for Safety/Refinement |
| :--- | :--- | :--- |
| **PromSec** (Prompt Optimization for Secure Generation) | Operates in an interactive loop where a fixed, improved code $c^{\prime}$ generated by a graph neural network (gGAN) is **reverse-engineered by the LLM** to produce an **improved prompt $p^{\prime}$**. | **gGAN (Graph Generative Adversarial Network):** Fixes and reduces security vulnerabilities in generated codes by altering code graph representations (like Control Flow Graphs/CFGs). **Dual-Objective Optimization:** Aims for both security (reduced Common Weakness Enumerations/CWE count) and functionality preservation (semantic similarity). |
| **RCI** (Recursive Criticism and Improvement) | The initial prompt generates code. Subsequent prompts contain the model's *self-critique* of the previous output and instructions to refine the code based on that feedback. | **Self-Critiquing Capabilities:** Leverages the LLM’s ability to analyze its own response, identify potential security issues, and then refine the output iteratively (typically two iterations were used in the evaluation). |
| **Prochemy** (Prompt Alchemy) | An automated approach that iteratively refines prompts by employing a **Mutation** process (rephrasing or altering task instructions to create variants). The final prompt is fixed and designed for consistent reuse. | **Execution-Driven Evaluation:** Refinement is based on the model’s performance evaluated against a training set by execution. **Weighted Scoring Mechanism:** Prioritizes complex tasks that are harder to solve correctly when selecting the best prompt for the next iteration. |
| **RCI+RAG** | Combines the prompt augmentation of RAG with the iterative refinement of RCI. The retrieved security guidelines are appended to the **initial prompt** used in the RCI process. | This method benefits from the complementary strengths of both techniques: the guidelines enhance the refinement capability of the LLM in RCI. |

***

### General Trend 3: Model Tuning for Secure Prompt Response

This trend focuses on structurally modifying the LLM itself during the instruction tuning phase so that it is inherently biased toward generating secure code when presented with a standard prompt/instruction during inference.

| Technique | Mechanism of Prompt Manipulation | Key Components for Safety |
| :--- | :--- | :--- |
| **SafeCoder** (Instruction Tuning) | This approach is focused on **security-centric fine-tuning** of the LLM, rather than manipulating the prompt at inference time. It trains the model to respond securely to functionality-only instructions. | **Joint Optimization:** Combines standard utility instruction tuning with security-specific tuning. **Masked Loss Functions:** Uses a masked negative log-likelihood loss on secure programs ($o_{sec}$) and a masked unlikelihood loss on vulnerable programs ($o_{vul}$), focusing the learning signal only on security-critical tokens. |

***

### Summary of General Trends

These diverse approaches demonstrate a concerted effort to mitigate the inherent tendency of LLMs to introduce security vulnerabilities, which often stems from their training on massive open-source corpora containing insecure programming practices.

1.  **Retrieval-Augmented Augmentation (RAG)** provides a training-free solution that injects necessary security knowledge into the prompt at runtime. Key to its effectiveness is creating highly relevant and concise knowledge snippets, as seen in **RESCUE's** use of program slicing and multi-faceted retrieval.
2.  **Iterative Refinement** leverages the LLM's own generative and analytical capabilities, repeatedly improving the code and generating updated prompts based on feedback. This feedback can be internal (self-critique in **RCI**) or external (graph analysis of fixed code in **PromSec**).
3.  **Model Tuning (SafeCoder)** represents a deep integration approach, permanently altering the LLM's response mechanism to ensure that standard user instructions result in functionally correct *and* secure code, achieving a "security-for-free" benefit.

The studies show that combining these strategies can yield superior performance, as evidenced by **RCI+RAG**, which combines prompt augmentation with refinement to achieve the best overall security results in its corresponding evaluation.